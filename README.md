# Engram

**Local-first memory server for AI agents.**

Engram indexes collections of [AIF-BIN](https://github.com/terronexdev/aifbin) semantic memory files and provides fast retrieval via HTTP API and MCP (Model Context Protocol) for AI agent integration.

## Features

- üîç **Semantic Search** ‚Äî Query memories by meaning using embedded vectors
- ‚ö° **Hybrid Retrieval** ‚Äî Combine vector similarity with keyword matching
- ü§ñ **MCP Server** ‚Äî Native integration with AI agents (Claude, OpenClaw, etc.)
- üåê **HTTP API** ‚Äî RESTful endpoints for any client
- üì¶ **Zero Cloud** ‚Äî Fully local, no external services required
- üóÇÔ∏è **Collection Management** ‚Äî Organize memories into logical groups

## Installation

```bash
# npm
npm install -g @terronex/engram

# bun
bun install -g @terronex/engram
```

## Quick Start

```bash
# Index a directory of .aif-bin files
engram index ./memories --collection my-project

# Start the server
engram serve

# Search via CLI
engram search "what decisions did we make about the API?"

# Or query the HTTP API
curl "http://localhost:3847/search?q=API+decisions&collection=my-project"
```

## MCP Integration

Add Engram to your AI agent's MCP config:

```json
{
  "mcpServers": {
    "engram": {
      "command": "engram",
      "args": ["mcp"]
    }
  }
}
```

Available MCP tools:
- `engram_search` ‚Äî Semantic search across collections
- `engram_recall` ‚Äî Retrieve specific memories by ID
- `engram_collections` ‚Äî List available collections
- `engram_ingest` ‚Äî Add new files to a collection

## HTTP API

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/search` | GET | Semantic search with optional filters |
| `/recall/:id` | GET | Retrieve specific memory chunk |
| `/collections` | GET | List all collections |
| `/collections/:name` | POST | Create/update collection |
| `/ingest` | POST | Add files to collection |
| `/health` | GET | Server health check |

## Configuration

```yaml
# ~/.engram/config.yaml
server:
  port: 3847
  host: localhost

index:
  path: ~/.engram/index.db
  
search:
  default_limit: 10
  hybrid_weight: 0.7  # 0 = keywords only, 1 = vectors only
```

## How It Works

1. **Indexing**: Engram reads `.aif-bin` files (created by [AIF-BIN Pro](https://github.com/terronexdev/aifbin)) and extracts their embedded vectors and text chunks into a local SQLite database.

2. **Search**: Queries are embedded using the same model, then matched against indexed vectors using cosine similarity. Optional BM25 keyword matching provides hybrid retrieval.

3. **Retrieval**: Results include the original text, metadata, source file, and similarity scores ‚Äî ready for RAG pipelines or direct AI consumption.

## Part of the AIF-BIN Ecosystem

- **[AIF-BIN](https://github.com/terronexdev/aifbin)** ‚Äî Core specification and SDKs
- **[AIF-BIN Pro](https://github.com/terronexdev/aifbin-pro)** ‚Äî CLI for creating .aif-bin files
- **Engram** ‚Äî Memory server for querying collections *(you are here)*

## License

MIT ¬© 2026 Terronex
